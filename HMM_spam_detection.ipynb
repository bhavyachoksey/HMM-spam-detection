{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUYdtpCUfKKG",
        "outputId": "830f70fa-38f4-423e-dde6-414b7f90edab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-8q4W39fL0y",
        "outputId": "29a6c180-bf87-45c9-e4d4-208630cd155f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from decimal import Decimal\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# use natural language toolkit\n",
        "import nltk\n",
        "#import stop word\n",
        "from nltk.corpus import stopwords\n",
        "#stemmer library import\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# word stemmer\n",
        "stemmer = LancasterStemmer()"
      ],
      "metadata": {
        "id": "I318y2JOfL3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding the csv file\n",
        "import chardet\n",
        "with open('data_for_spam.csv', 'rb') as f:\n",
        "    result = chardet.detect(f.read())  # or readline if the file is large\n",
        "dataset=pd.read_csv('data_for_spam.csv', encoding=result['encoding'])\n",
        "#splitting thr csv file into A and y\n",
        "x=dataset.iloc[:,0]\n",
        "y=dataset.iloc[:,1]\n",
        "X=x.to_dict()"
      ],
      "metadata": {
        "id": "NZ0Zorv9fL6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)\n",
        "\n",
        "#index acending\n",
        "s=y_train.reset_index()\n",
        "y_train=s.iloc[:,1]"
      ],
      "metadata": {
        "id": "D61f4msvfL8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class wise data entry in training_data\n",
        "training_data = []\n",
        "for d in range(len(X_train)):\n",
        "    #spam class training_data\n",
        "    if y_train[d]==\"spam\":\n",
        "        training_data.append({ \"class\":\"spam\",\"sentence\":X_train[d]})\n",
        "    #ham class training_data\n",
        "    else:\n",
        "        training_data.append({ \"class\":\"ham\",\"sentence\":X_train[d]})\n",
        "        #length of thr trainig_data\n",
        "print (\"%s sentences of training data\" % len(training_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y4KVvC7fZkh",
        "outputId": "a06bfea4-b56e-4e70-ce26-513219bbd370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70 sentences of training data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#class dictinery\n",
        "class_words = {}\n",
        "\n",
        "# turn a list into a set (of unique items) and then a list again (this removes duplicates)\n",
        "classes = list(set([a['class'] for a in training_data]))\n",
        "for c in classes:\n",
        "    # prepare a list of words within each class\n",
        "    class_words[c] = []"
      ],
      "metadata": {
        "id": "zEr4MwVDfcbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spam and ham word list creat\n",
        "sp_word=[]\n",
        "hm_word=[]\n",
        "cr_words=[]\n",
        "for data in training_data:\n",
        "    sen=data['sentence']\n",
        "    #temporarry list for appending to word list\n",
        "    cr_words=[]\n",
        "    #removing digit and punctuation\n",
        "    sentence= re.sub(r'\\d+','', sen)\n",
        "    sentence= re.sub('['+string.punctuation+']', '', sentence)\n",
        "    #spam word lists\n",
        "    if data['class']=='spam':\n",
        "        for word in nltk.word_tokenize(sentence):\n",
        "            if word not in stop_words:\n",
        "                stemmed_word = stemmer.stem(word.lower())\n",
        "                cr_words.append(stemmed_word)\n",
        "        #sentence word count\n",
        "        from collections import Counter\n",
        "        count=Counter(cr_words)\n",
        "        #decending by value\n",
        "        w=[(l,k) for k,l in sorted([(j,i) for i,j in count.items()], reverse=True)]\n",
        "        w=np.array(w)\n",
        "        #just word list taken in a decinding way\n",
        "        B=w[:,0] #we are converting the list of tuples w into a NumPy array and then extract only the words (the first column) into the B variable.\n",
        "        #forming a spam word list\n",
        "        sp_word.append(B)\n",
        "    else:\n",
        "        #spam word lists\n",
        "        for word in nltk.word_tokenize(sentence):\n",
        "            if word not in stop_words:\n",
        "                stemmed_word = stemmer.stem(word.lower())\n",
        "                cr_words.append(stemmed_word)\n",
        "\n",
        "        #sentence word count\n",
        "        from collections import Counter\n",
        "        count=Counter(cr_words)\n",
        "        w=[(l,k) for k,l in sorted([(j,i) for i,j in count.items()], reverse=True)]\n",
        "        w=np.array(w)\n",
        "        #just word list taken in a decinding wa\n",
        "        B=w[:,0]\n",
        "        #forming a spam word list\n",
        "        hm_word.append(B)"
      ],
      "metadata": {
        "id": "4FJYxOzDfcd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#forming a ham matrix\n",
        "for i in range(1):\n",
        "    #total list of ham_row in a column then find the max length a which will be the row dimention\n",
        "    a=[]\n",
        "    cl_word=hm_word\n",
        "    hm_word=[]\n",
        "    for j in range(len(cl_word)):\n",
        "        a.append(len(cl_word[j]))\n",
        "    for j in range(len(cl_word)):\n",
        "        n=cl_word[j].tolist()\n",
        "        #finding highest row length\n",
        "        w=max(a)\n",
        "        for k in range(w):\n",
        "            if k==len(cl_word[j]):\n",
        "                b=len(cl_word[j])+1\n",
        "                #append rest of by appending to\n",
        "                for l in range(b,w+1):\n",
        "                    #to added cause its  in stop word\n",
        "                    n.append('None')\n",
        "        hm_word.append(n)\n",
        "hm_word_matrix=np.array(hm_word)\n",
        "###end forming the ham word matrix8\n",
        "\n",
        "#forming a spam matrix\n",
        "for i in range(1):\n",
        "    #total list of spam_row in a column then find the max length a which will be the row dimention\n",
        "    a=[]\n",
        "    cl_word=sp_word\n",
        "    sp_word=[]\n",
        "    for j in range(len(cl_word)):\n",
        "        a.append(len(cl_word[j]))\n",
        "    for j in range(len(cl_word)):\n",
        "        n=cl_word[j].tolist()\n",
        "        #finding highest row length\n",
        "        w=max(a)\n",
        "        for k in range(w):\n",
        "            if k==len(cl_word[j]):\n",
        "                b=len(cl_word[j])+1\n",
        "                #append rest of by appending to\n",
        "                for l in range(b,w+1):\n",
        "                    n.append('None')\n",
        "        sp_word.append(n)\n",
        "#convert list to array\n",
        "sp_word_matrix=np.array(sp_word)\n",
        "###end forming the spam word matrix"
      ],
      "metadata": {
        "id": "ykf6Sozkfcf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#smoothing factor\n",
        "f=.15"
      ],
      "metadata": {
        "id": "IJRn07BOf0O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#states calculaton for ham_word_matrix\n",
        "hm_obs=0\n",
        "for i in range(len(hm_word)):\n",
        "    a=len(hm_word[i])\n",
        "    for j in range(a):\n",
        "        hm_obs+=1\n",
        "hm_states=int((hm_obs/len(hm_word)))\n",
        "\n",
        "#states calculaton for spam_word_matrix\n",
        "sp_obs=0\n",
        "for i in range(len(sp_word)):\n",
        "    a=len(sp_word[i])\n",
        "    for j in range(a):\n",
        "        sp_obs+=1\n",
        "sp_states=int((sp_obs/len(sp_word)))"
      ],
      "metadata": {
        "id": "vc4gXE1jf0Um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ith sate probabilty for any classes\n",
        "def state_ith_prob(wd,cr_word,cr_word_matrix,state):\n",
        "    c_R_d=[]\n",
        "    count_R_d=0\n",
        "    for i in range(1):\n",
        "        a=len(cr_word)\n",
        "        for k in range(a):\n",
        "            if wd==cr_word_matrix[k][state]:\n",
        "                c_R_d.append(1)\n",
        "            count_R_d+=1\n",
        "    return len(c_R_d),count_R_d"
      ],
      "metadata": {
        "id": "yFIQqDlxf0ZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#n_states probability\n",
        "def state_tot_prob(wd,cr_word,cr_word_matrix):\n",
        "    c_A_d=[]\n",
        "    count_A_d=0\n",
        "    for i in range(len(cr_word)):\n",
        "        a=len(cr_word[i])\n",
        "        for k in range(a):\n",
        "            if wd==cr_word_matrix[i][k]:\n",
        "                c_A_d.append(1)\n",
        "            count_A_d+=1\n",
        "    return len(c_A_d),count_A_d"
      ],
      "metadata": {
        "id": "AV3Db9puf0cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hmm model\n",
        "def hmm(w,cl,state,cr_word,cr_word_matrix):\n",
        "    if cl=='spam':\n",
        "        R=state_ith_prob(w,cr_word,cr_word_matrix,state)\n",
        "        A=state_tot_prob(w,cr_word,cr_word_matrix)\n",
        "        R_d,R_ln=R\n",
        "        A_d,A_ln=A\n",
        "        score=(f*(R_d/R_ln)+(1-f)*(A_d/A_ln))\n",
        "        #The probability of finding the word w at the specified state (f * (R_d / R_ln)).\n",
        "#The probability of finding the word w in the entire word matrix for the class ((1 - f) * (A_d / A_ln)).\n",
        "    else:\n",
        "        R=state_ith_prob(w,cr_word,cr_word_matrix,state)\n",
        "        A=state_tot_prob(w,cr_word,cr_word_matrix)\n",
        "        R_d,R_ln=R\n",
        "        A_d,A_ln=A\n",
        "        score=(f*(R_d/R_ln)+(1-f)*(A_d/A_ln))\n",
        "        #specific prob vs overall prob\n",
        "    return score"
      ],
      "metadata": {
        "id": "SSY8IWjqf-GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transition matrix for ham\n",
        "# y is transition list\n",
        "y=[]\n",
        "for i in range (hm_states):\n",
        "    #row list appended in y\n",
        "    a=[]\n",
        "    for j in range (hm_states):\n",
        "        #i=j+1 emitted\n",
        "        if j==i+1:\n",
        "            a.append(1)\n",
        "        else:\n",
        "            a.append(0)\n",
        "    y.append(a)\n",
        "#y_list to array\n",
        "hm_transition_matrix=np.array(y)\n",
        "\n",
        "#transition matrix for spam\n",
        "# y is transition list\n",
        "y=[]\n",
        "for i in range (sp_states):\n",
        "    #row list appended in y\n",
        "    a=[]\n",
        "    for j in range (sp_states):\n",
        "        #i=j+1 emitted\n",
        "        if j==i+1:\n",
        "            a.append(1)\n",
        "        else:\n",
        "            a.append(0)\n",
        "    y.append(a)\n",
        "#y_list to array\n",
        "sp_transition_matrix=np.array(y)"
      ],
      "metadata": {
        "id": "DA5f8Mjlf-Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#learning_Rate\n",
        "n=10000"
      ],
      "metadata": {
        "id": "Dwly97nNf-PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classify the test sentences\n",
        "def classify(sentence):\n",
        "    sentence= re.sub(r'\\d+','', sentence)\n",
        "    sentence= re.sub('['+string.punctuation+']', '', sentence)\n",
        "    sentence=sentence.lower()\n",
        "    tokens=[]\n",
        "    for word in nltk.word_tokenize(sentence):\n",
        "        if word not in stop_words:\n",
        "            stemmed_word = stemmer.stem(word.lower())\n",
        "            tokens.append(stemmed_word)\n",
        "\n",
        "    high_class = None\n",
        "    high_score =0\n",
        "    # loop through our classes\n",
        "    for c in classes:\n",
        "        # calculate score of sentence for each class\n",
        "        if c=='ham':\n",
        "            alpha=0\n",
        "            sum=0\n",
        "            #header loop for calculating alpha_T\n",
        "            for i in  range(hm_states):\n",
        "                #individual word in tokens and alpha_T count\n",
        "                for word in tokens:\n",
        "                    #word probabilty\n",
        "                    a=hmm(word,c,i,hm_word,hm_word_matrix)\n",
        "#                    print(a)\n",
        "                    #sates probabilty\n",
        "                    for j in range(hm_states):\n",
        "                        #at first state initial probability 1\n",
        "                        if 0==i:\n",
        "                            r=1\n",
        "                        #r after state one states probability\n",
        "                        else:\n",
        "                            for j in range(hm_states):\n",
        "                                #emitted only i=j+1\n",
        "                                sum=sum+b*(hm_transition_matrix[j][i])\n",
        "                            r=sum\n",
        "                    #o1,o2,......oT probability\n",
        "                    b=r*a\n",
        "                    #i_th probability of a sentence\n",
        "                    #alpha_T at the end\n",
        "                    alpha=alpha+b\n",
        "            score=alpha\n",
        "            score=n*(score)\n",
        "        else:\n",
        "            c=='spam'\n",
        "            alpha=0\n",
        "            sum=0\n",
        "            for i in  range(sp_states):\n",
        "                for word in tokens:\n",
        "\n",
        "                    a=hmm(word,c,i,sp_word,sp_word_matrix)\n",
        "                    for j in range(hm_states):\n",
        "                        if 0==i:\n",
        "                            r=1\n",
        "                        else:\n",
        "                            for j in range(sp_states):\n",
        "                                sum=sum+b*sp_transition_matrix[j][i]\n",
        "                            r=sum\n",
        "                    b=r*a\n",
        "                    alpha=alpha+b\n",
        "            #score global variable for comparision\n",
        "            score=alpha\n",
        "            #n smoothing factor\n",
        "            score=n*(score)\n",
        "        #determine class according to score\n",
        "        if score > high_score:\n",
        "            high_class = c\n",
        "            high_score = score\n",
        "    return high_class, high_score"
      ],
      "metadata": {
        "id": "h5e-Esgpf-V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_test calculation\n",
        "z=[]\n",
        "for j in range(len(X_test)):\n",
        "    z.append(classify(X_test[j]))\n",
        "#list to series\n",
        "Z= pd.Series( (v[0] for v in z))\n",
        "y_pred=Z"
      ],
      "metadata": {
        "id": "K9hHbHp5gMqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#confusion metrix\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "Accuracy_Score = accuracy_score(y_test, y_pred)\n",
        "Precision = precision_score(y_test, y_pred, average='weighted')\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n",
        "#The y_test dataset is used for evaluation to compare the model's predictions (in y_pred) against the actual ground truth labels.\n"
      ],
      "metadata": {
        "id": "oS57KZW9gMwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have already classified the test data and stored the predictions in y_pred\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Calculate accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy Score:\", accuracy)\n",
        "\n",
        "# Calculate precision score\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "print(\"Precision Score:\", precision)\n",
        "num_spam_calls = sum(1 for label in y_pred if label == 'spam')\n",
        "\n",
        "print(\"Number of Spam Calls:\", num_spam_calls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mVy4g3XgM1U",
        "outputId": "74c0ebc5-aaf5-4f24-d314-5c7000f83521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[16  3]\n",
            " [ 1 10]]\n",
            "Accuracy Score: 0.8666666666666667\n",
            "Precision Score: 0.8781297134238311\n",
            "Number of Spam Calls: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "userinput=input('enter text')\n",
        "final=classify(userinput)\n",
        "print(final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BK_jX-dtgWky",
        "outputId": "d9871b0d-125b-49fb-f577-b0d942288502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter textWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
            "('spam', 5.204455643676987e+30)\n"
          ]
        }
      ]
    }
  ]
}